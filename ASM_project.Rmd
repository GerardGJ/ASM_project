---
title: "ASM_project"
author: "Biel and Gerard"
date: "2023-12-21"
output: html_document
---

For this project we have decided to use the eurodol dataset. 

```{r}
serie=ts((read.table("Time Series Repository-20231107/eurodol.dat")),start=1995,freq=12)
plot(serie)
title("EURODOL")
abline(v=1995:2020,col=4,lty=3)
```

## Identification 

For this fist part of the project we are first going to the time series stationary, and then will analyse the ACF and PACF plot to identify the plausible models. 

The first step will be to see if the variance is constant:
```{r}
par(mfrow =c(1,2))
plot(serie)
title("EURODOL")
abline(v=1995:2020,col=4,lty=3)
plot(log(serie))
title("EURODOL")
abline(v=1995:2020,col=4,lty=3)
```

```{r}
par(mfrow =c(1,2))
boxplot(serie~floor(time(serie)))
title("EURODOL")

boxplot(log(serie)~floor(time(serie)))
title("EURODOL")
```

```{r}
m<-apply(matrix(serie,nrow=12),2,mean)
v<-apply(matrix(serie,nrow=12),2,var)
plot(v~m)
```
  
Looking at the different plots, we an see that the variance is already constant. This is clearly seen in the mean vs variance plot, we can see that for all means the variance is between 0 and 0.004. So there is no need to change the scale of the data in order to make variance constant, as it already is.

The next step it is to look for the seasonal difference. As we have monthly data if there is a seasonality we will find it every 12 observations.Our first step in order to identify the seasonality will be to plot the month plot.

```{r}
monthplot(serie)
```
  
At fist glance, from the month plot we can conclude that there is no seasonality, as the mean stays more or less constant all through the months. 
 

```{r}
ts.plot(matrix(serie,nrow=12),col=1:8)
```

Looking at this this plot, we can again clearly see that there is no seasonality as all the lines are crossing each other, and they are not following any pattern.

The next step is the constant mean:
```{r}
plot(serie)
abline(h=mean(serie))
```



```{r}
d1serie <- diff(serie)
plot(d1serie)
abline(h=0)
```
```{r}
d1d1serie <- diff(d1serie)
plot(d1d1serie)
abline(h=0)
```

```{r}
var(serie)
var(d1serie)
var(d1d1serie) 
```
We can see that having two differenciations increases the variance, so the lowest variance is achieved after one differentiation on the series.

The next step is to analyse the ACF and PACF plots to determine which could be the plausible plots:
```{r}
par(mfrow=c(1,2))
acf(d1serie)
pacf(d1serie)
```
Some plausible models could be MA(1), AR(1), and also we could consider an MA(7)

```{r}
(model1 <- arima(serie, c(0,1,1))) #MA(1) model
```


```{r}
(model2 <- arima(serie, c(1,1,0))) #AR(1) model
```


```{r}
(model3 <- arima(serie, c(1,1,1))) #ARMA(1,1) model
```
ARIMA 1 1 is not significant

## Model validation:
```{r}
#################Validation#################################
validation=function(model,dades){
  s=frequency(get(model$series))
  resid=model$residuals
  par(mfrow=c(2,2),mar=c(3,3,3,3))
  #Residuals plot
  plot(resid,main="Residuals")
  abline(h=0)
  abline(h=c(-3*sd(resid),3*sd(resid)),lty=3,col=4)
  #Square Root of absolute values of residuals (Homocedasticity)
  scatter.smooth(sqrt(abs(resid)),main="Square Root of Absolute residuals",
                 lpars=list(col=2))
  
  #Normal plot of residuals
  qqnorm(resid)
  qqline(resid,col=2,lwd=2)
  
  ##Histogram of residuals with normal curve
  hist(resid,breaks=20,freq=FALSE)
  curve(dnorm(x,mean=mean(resid),sd=sd(resid)),col=2,add=T)
  
  
  #ACF & PACF of residuals
  par(mfrow=c(1,2))
  acf(resid,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=1)
  pacf(resid,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=1)
  par(mfrow=c(1,1))
  
  #ACF & PACF of square residuals 
  par(mfrow=c(1,2))
  acf(resid^2,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=1)
  pacf(resid^2,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=1)
  par(mfrow=c(1,1))
  
  #Ljung-Box p-values
  par(mar=c(2,2,1,1))
  tsdiag(model,gof.lag=7*s)
  cat("\n--------------------------------------------------------------------\n")
  print(model)
  
  #Stationary and Invertible
  cat("\nModul of AR Characteristic polynomial Roots: ", 
      Mod(polyroot(c(1,-model$model$phi))),"\n")
  cat("\nModul of MA Characteristic polynomial Roots: ",
      Mod(polyroot(c(1,model$model$theta))),"\n")
  
  #Model expressed as an MA infinity (psi-weights)
  psis=ARMAtoMA(ar=model$model$phi,ma=model$model$theta,lag.max=36)
  names(psis)=paste("psi",1:36)
  cat("\nPsi-weights (MA(inf))\n")
  cat("\n--------------------\n")
  print(psis[1:20])
  
  #Model expressed as an AR infinity (pi-weights)
  pis=-ARMAtoMA(ar=-model$model$theta,ma=-model$model$phi,lag.max=36)
  names(pis)=paste("pi",1:36)
  cat("\nPi-weights (AR(inf))\n")
  cat("\n--------------------\n")
  print(pis[1:20])
  
  ## Add here complementary tests (use with caution!)
  ##---------------------------------------------------------
  cat("\nNormality Tests\n")
  cat("\n--------------------\n")
 
  ##Shapiro-Wilks Normality test
  print(shapiro.test(resid(model)))

  suppressMessages(require(nortest,quietly=TRUE,warn.conflicts=FALSE))
  ##Anderson-Darling test
  print(ad.test(resid(model)))
  
  suppressMessages(require(tseries,quietly=TRUE,warn.conflicts=FALSE))
  ##Jarque-Bera test
  print(jarque.bera.test(resid(model)))
  
  cat("\nHomoscedasticity Test\n")
  cat("\n--------------------\n")
  suppressMessages(require(lmtest,quietly=TRUE,warn.conflicts=FALSE))
  ##Breusch-Pagan test
  obs=get(model$series)
  print(bptest(resid(model)~I(obs-resid(model))))
  
  cat("\nIndependence Tests\n")
  cat("\n--------------------\n")
  
  ##Durbin-Watson test
  print(dwtest(resid(model)~I(1:length(resid(model)))))
  
  ##Ljung-Box test
  cat("\nLjung-Box test\n")
  print(t(apply(matrix(c(1:4,(1:4)*s)),1,function(el) {
    te=Box.test(resid(model),type="Ljung-Box",lag=el)
    c(lag=(te$parameter),statistic=te$statistic[[1]],p.value=te$p.value)})))
  

  #Sample ACF vs. Teoric ACF
  par(mfrow=c(2,2),mar=c(3,3,3,3))
  acf(dades, ylim=c(-1,1) ,lag.max=36,main="Sample ACF")
  
  plot(ARMAacf(model$model$phi,model$model$theta,lag.max=36),ylim=c(-1,1), 
       type="h",xlab="Lag",  ylab="", main="ACF Teoric")
  abline(h=0)
  
  #Sample PACF vs. Teoric PACF
  pacf(dades, ylim=c(-1,1) ,lag.max=36,main="Sample PACF")
  
  plot(ARMAacf(model$model$phi,model$model$theta,lag.max=36, pacf=T),ylim=c(-1,1),
       type="h", xlab="Lag", ylab="", main="PACF Teoric")
  abline(h=0)
  par(mfrow=c(1,1))
}
```
### Model 1 validation:
```{r}
validation(model1,d1serie)
```
### Model 1 stability
```{r}
last=c(2018,12)
pdq=c(0,1,1)

serie2=window(serie,end=last)

(mod.1=arima(serie,order=pdq))
(mod.2=arima(serie2,order=pdq))
```

```{r}
pred=predict(mod.2,n.ahead=12)
pr<-ts(c(tail(serie2,1),pred$pred),start=last,freq=12)

se<-ts(c(0,pred$se),start=last,freq=12)

#Intervals
tl<-ts(pr-1.96*se,start=last,freq=12)
tu<-ts(pr+1.96*se,start=last,freq=12)
pr<-ts(pr,start=last,freq=12)

ts.plot(serie,tl,tu,pr,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=last[1]+c(-3,+2),type="o",main="Model ARIMA(0,1,1)")
abline(v=(last[1]-3):(last[1]+2),lty=3,col=4)
```


### Model 2 validation:
```{r}
validation(model2)
```

